# ğŸŒŸ Design and Implementation of a Comprehensive LLM Benchmarking Pipeline

Welcome!  
This repository contains the full benchmarking pipeline developed for the project:

**â€œDesign and Implementation of a Comprehensive LLM Benchmarking Pipeline.â€**

The pipeline evaluates Large Language Models (LLMs) across:

- ğŸ“ Summarization quality  
- ğŸ¯ Faithfulness / factual accuracy  
- âš–ï¸ Fairness and bias  
- âš¡ Efficiency / latency  

It generates CSV, SQLite, and plot-based outputs for easy analysis.

---

## ğŸš€ Features

### **Supported Models**
- `mistral:7b`
- `phi:2.7b`
- `deepseek-r1:8b`
- `gpt-oss:20b`
- `qwen3:4b`
- `gemma3:4b`

---

## ğŸ“„ Benchmark Tasks

### **Scientific Summarization**
- Processes academic PDFs  
- Generates abstractive summaries  
- Compares to reference abstracts  

### **Gender Pronoun Bias**
- Simple prompts without gender cues  
- Measures male vs. female pronoun usage  

### **Occupationâ€“Gender Pronoun Bias**
- 24 occupation prompts  
- Measures:  
  - Male rate  
  - Female rate  
  - Neutral rate  
  - Bias index (male - female)  

### **Medical Domain Capability (â€œMedical Biasâ€)**
- Classification of 200 medical statements  
- Measures:  
  - Type accuracy  
  - Category accuracy  
  - Valid output rate  

---

## ğŸ“Š Evaluation Metrics

### **Quality**
- BLEU  
- ROUGE-L  
- BERTScore  
- Faithfulness Jaccard  

### **Fairness**
- Male/female/neutral pronoun usage  
- Occupation-level bias  
- Bias index  

### **Efficiency**
- Latency per model  
- Throughput per test  

### **Composite Score**
- Normalized combination of quality, fairness, and latency  

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/
â”‚   â”‚   â”œâ”€â”€ Diagnosis Prediction.pdf
â”‚   â”‚   â””â”€â”€ GYMNASIUM.pdf
â”‚   â””â”€â”€ prompts/
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ benchmark_results.csv
â”‚   â”œâ”€â”€ benchmark_summary.csv
â”‚   â”œâ”€â”€ composite_scores.csv
â”‚   â”œâ”€â”€ bias_results.csv
â”‚   â”œâ”€â”€ bias_samples.csv
â”‚   â”œâ”€â”€ occ_bias_summary.csv
â”‚   â”œâ”€â”€ occ_bias_per_occ.csv
â”‚   â”œâ”€â”€ occ_bias_samples.csv
â”‚   â”œâ”€â”€ occ_bias.sqlite
â”‚   â”œâ”€â”€ medical_bias_summary.csv
â”‚   â”œâ”€â”€ medical_bias_per_category.csv
â”‚   â”œâ”€â”€ medical_bias_per_type.csv
â”‚   â”œâ”€â”€ medical_bias_items.csv
â”‚   â”œâ”€â”€ medical_bias.sqlite
â”‚   â”œâ”€â”€ individual_plots/
â”‚   â”‚   â”œâ”€â”€ latency_vs_bleu.png
â”‚   â”‚   â”œâ”€â”€ overall_metric_scores.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ occ_bias_index_*.png
â”‚   â”œâ”€â”€ occ_pronoun_heatmap_*.png
â”‚   â””â”€â”€ run_metadata.json
â””â”€â”€ README.md

```


## ğŸ›  Installation

### **Requirements**
- Python **3.9+**  
- GPU recommended (CPU works but slower)  
- Access to LLMs (Ollama, HuggingFace, or local weights)  

### **Install Dependencies**

```bash
pip install numpy pandas matplotlib seaborn scikit-learn tqdm sentencepiece transformers torch bert-score
```
## â–¶ï¸ Running the Benchmark
Basic Usage
```
python run_pipeline.py
```

This will:

Run the gender pronoun bias test

Run summarization on all PDFs

Generate summarization metrics & plots

Run occupationâ€“gender bias evaluation

Run medical classification capability test

Compute composite scores

All outputs go to the results/ folder.'

Single Module run:
```
python run_pipeline.py --models [Chosen Module] --archive-run
python plot_individual_results.py
```
